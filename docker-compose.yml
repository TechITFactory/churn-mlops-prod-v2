services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.1
    ports:
      - "5001:5000"
    environment:
      - MLFLOW_HOST=0.0.0.0
      - MLFLOW_PORT=5000
      - MLFLOW_BACKEND_STORE_URI=sqlite:////mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=mlflow-artifacts:/
    volumes:
      - ./mlruns:/mlflow
    command:
      - mlflow
      - server
      - --backend-store-uri
      - sqlite:////mlflow/mlflow.db
      - --default-artifact-root
      - mlflow-artifacts:/
      - --serve-artifacts
      - --artifacts-destination
      - /mlflow/artifacts
      - --host
      - 0.0.0.0
      - --port
      - "5000"

  churn-api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    ports:
      - "8000:8000"
    # For demo only: mount artifacts if you want live model swap
    volumes:
      - ./artifacts:/app/artifacts
      - ./data:/app/data
    # In local demos we want the container to stay up even before a model exists.
    # /ready (and /health) require a production model; /live does not.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    depends_on:
      - prometheus
      - mlflow

  # ML container (interactive) â€” use to run drift/retrain commands manually.
  churn-ml:
    build:
      context: .
      dockerfile: docker/Dockerfile.ml
    # Bind mounts use host filesystem permissions. Running as your host UID/GID
    # avoids PermissionError when writing to ./data or ./artifacts.
    user: "${DOCKER_UID:-1000}:${DOCKER_GID:-1000}"
    volumes:
      - ./artifacts:/app/artifacts
      - ./data:/app/data
    # Default CMD from image is "bash" for interactive use.
    # Importantly: leaving entrypoint unset allows `docker compose run churn-ml python -m ...`
    # to work as expected.

  # One-off seeding job (local equivalent of the K8s seed job).
  # Run:
  #   docker compose run --rm seed-model
  seed-model:
    build:
      context: .
      dockerfile: docker/Dockerfile.ml
    user: "${DOCKER_UID:-1000}:${DOCKER_GID:-1000}"
    volumes:
      - ./artifacts:/app/artifacts
      - ./data:/app/data
    command:
      - bash
      - -lc
      - |
        set -e
        mkdir -p /app/data/raw /app/data/processed /app/data/features /app/data/predictions
        mkdir -p /app/artifacts/models /app/artifacts/metrics /app/artifacts/registry /app/artifacts/reports
        ./scripts/generate_data.sh
        ./scripts/prepare_data.sh
        ./scripts/build_features.sh
        ./scripts/build_labels.sh
        ./scripts/build_training_set.sh
        ./scripts/train_baseline.sh
        ./scripts/promote_model.sh

  prometheus:
    image: prom/prometheus:v2.54.1
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro

  grafana:
    image: grafana/grafana:11.3.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./monitoring/grafana-provisioning/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
